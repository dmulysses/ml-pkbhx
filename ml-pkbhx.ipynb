{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "import catboost as ct\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import xgboost as xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "data = pd.read_csv('FILE_PATH\\dataset-pkbhx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Is there any null values?\n",
    "print(data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = data[['pkbhx']] # Dataframe with the target\n",
    "data_X = data.drop(columns=['SMILES', 'pkbhx']) # Dataframe with the features\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(data_X, data_y, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sca = StandardScaler()\n",
    "X_train_sca = pd.DataFrame(\n",
    "    sca.fit_transform(X_train.drop(['id'], axis=1)),\n",
    "    columns=X_train.drop(['id'], axis=1).columns\n",
    ")\n",
    "X_test_sca = pd.DataFrame(\n",
    "    sca.transform(X_test.drop(['id'], axis=1)),\n",
    "    columns=X_test.drop(['id'], axis=1).columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train_sca.values, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reg.predict(X_test_sca.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "def objectiveRF(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'absolute_error', 'friedman_mse']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**params, random_state=0)\n",
    "    model.fit(X_train_sca.values, y_train.values.ravel())\n",
    "    predictions = model.predict(X_test_sca.values)\n",
    "    rmse = root_mean_squared_error(y_test.values.ravel(), predictions)\n",
    "    return rmse\n",
    "\n",
    "studyRF = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyRF.optimize(objectiveRF, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print('Best hyperparameters:', studyRF.best_params)\n",
    "print('Best RMSE:', studyRF.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor(random_state=0, n_estimators=393, criterion='absolute_error', max_depth=10, min_samples_split=2, min_samples_leaf=1, max_features='log2')\n",
    "\n",
    "model_rf.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_rf.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out + K-Fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "def objectiveRF(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'absolute_error', 'friedman_mse']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**params, random_state=0)\n",
    "\n",
    "    # Compute MAE and R2 for analysis\n",
    "    mae_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(model,  X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='r2')\n",
    "\n",
    "    # Store MAE and R2 scores as user attributes\n",
    "    trial.set_user_attr('mae_scores', mae_scores)\n",
    "    trial.set_user_attr('r2_scores', r2_scores)\n",
    "\n",
    "    # Compute neg_root_mean_squared_error for optimization\n",
    "    rmse_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_root_mean_squared_error')\n",
    "    trial.set_user_attr('rmse_scores', rmse_scores)\n",
    "\n",
    "    return rmse_scores.mean()\n",
    "\n",
    "studyRF = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyRF.optimize(objectiveRF, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "try:\n",
    "    best_rmse_scores = studyRF.best_trial.user_attrs['rmse_scores']\n",
    "    best_mae_scores = studyRF.best_trial.user_attrs['mae_scores']\n",
    "    best_r2_scores = studyRF.best_trial.user_attrs['r2_scores']\n",
    "\n",
    "    mean_rmse = np.mean(best_rmse_scores)\n",
    "    std_rmse = np.std(best_rmse_scores)\n",
    "    mean_mae = np.mean(best_mae_scores)\n",
    "    std_mae = np.std(best_mae_scores)\n",
    "    mean_r2 = np.mean(best_r2_scores)\n",
    "    std_r2 = np.std(best_r2_scores)\n",
    "\n",
    "    print(\"Mean RMSE of Best Model:\", mean_rmse)\n",
    "    print(\"Standard Deviation of RMSE of Best Model:\", std_rmse)\n",
    "    print(\"Mean MAE of Best Model:\", mean_mae)\n",
    "    print(\"Standard Deviation of MAE of Best Model:\", std_mae)\n",
    "    print(\"Mean R2 of Best Model:\", mean_r2)\n",
    "    print(\"Standard Deviation of R2 of Best Model:\", std_r2)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"No completed trials yet.\")\n",
    "\n",
    "print('Best hyperparameters:', studyRF.best_params)\n",
    "print('Best RMSE:', studyRF.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor(random_state=0, n_estimators=266, criterion='absolute_error', max_depth=10, min_samples_split=3, min_samples_leaf=1, max_features='sqrt')\n",
    "\n",
    "model_rf.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_rf.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model\n",
    "def objectiveXGB(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.05, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.05, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "    }\n",
    "\n",
    "    model = xgboost.XGBRegressor(**params, random_state=0, verbosity=0)\n",
    "    model.fit(X_train_sca.values, y_train.values.ravel(), verbose=False)\n",
    "    predictions = model.predict(X_test_sca.values)\n",
    "    rmse = root_mean_squared_error(y_test.values.ravel(), predictions)\n",
    "    return rmse\n",
    "\n",
    "studyXGB = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyXGB.optimize(objectiveXGB, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print('Best hyperparameters:', studyXGB.best_params)\n",
    "print('Best RMSE:', studyXGB.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = XGBRegressor(random_state=0, n_estimators=185, learning_rate=0.02596425441970596, \n",
    "                         max_depth=10, subsample=0.8969848974548218, colsample_bytree=0.8607583706114562, min_child_weight=5)\n",
    "\n",
    "model_xgb.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_xgb.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out + K-Fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model\n",
    "def objectiveXGB(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.05, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.05, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "    }\n",
    "\n",
    "    model = xgboost.XGBRegressor(**params, random_state=0, verbosity=0)\n",
    "\n",
    "    # Compute MAE and R2 for analysis\n",
    "    mae_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(model,  X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='r2')\n",
    "\n",
    "    # Store MAE and R2 scores as user attributes\n",
    "    trial.set_user_attr('mae_scores', mae_scores)\n",
    "    trial.set_user_attr('r2_scores', r2_scores)\n",
    "\n",
    "    # Compute neg_root_mean_squared_error for optimization\n",
    "    rmse_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_root_mean_squared_error')\n",
    "    trial.set_user_attr('rmse_scores', rmse_scores)\n",
    "\n",
    "    return rmse_scores.mean()\n",
    "\n",
    "studyXGB = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyXGB.optimize(objectiveXGB, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "try:\n",
    "    best_rmse_scores = studyXGB.best_trial.user_attrs['rmse_scores']\n",
    "    best_mae_scores = studyXGB.best_trial.user_attrs['mae_scores']\n",
    "    best_r2_scores = studyXGB.best_trial.user_attrs['r2_scores']\n",
    "\n",
    "    mean_rmse = np.mean(best_rmse_scores)\n",
    "    std_rmse = np.std(best_rmse_scores)\n",
    "    mean_mae = np.mean(best_mae_scores)\n",
    "    std_mae = np.std(best_mae_scores)\n",
    "    mean_r2 = np.mean(best_r2_scores)\n",
    "    std_r2 = np.std(best_r2_scores)\n",
    "\n",
    "    print(\"Mean RMSE of Best Model:\", mean_rmse)\n",
    "    print(\"Standard Deviation of RMSE of Best Model:\", std_rmse)\n",
    "    print(\"Mean MAE of Best Model:\", mean_mae)\n",
    "    print(\"Standard Deviation of MAE of Best Model:\", std_mae)\n",
    "    print(\"Mean R2 of Best Model:\", mean_r2)\n",
    "    print(\"Standard Deviation of R2 of Best Model:\", std_r2)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"No completed trials yet.\")\n",
    "\n",
    "print('Best hyperparameters:', studyXGB.best_params)\n",
    "print('Best RMSE:', studyXGB.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = XGBRegressor(random_state=0, n_estimators=967, learning_rate=0.010010842387401658, \n",
    "                         max_depth=8, subsample=0.29251392417671074, colsample_bytree=0.8129074263707009, min_child_weight=1)\n",
    "\n",
    "model_xgb.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_xgb.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Model\n",
    "def objectiveCT(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10, log=True),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide'])\n",
    "    }\n",
    "\n",
    "    model = ct.CatBoostRegressor(**params, random_state=0, silent=True, eval_metric='RMSE')\n",
    "    model.fit(X_train_sca.values, y_train.values.ravel())\n",
    "    predictions = model.predict(X_test_sca.values)\n",
    "    rmse = root_mean_squared_error(y_test.values.ravel(), predictions)\n",
    "    return rmse\n",
    "\n",
    "studyCT = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyCT.optimize(objectiveCT, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print('Best hyperparameters:', studyCT.best_params)\n",
    "print('Best RMSE:', studyCT.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ct = ct.CatBoostRegressor(n_estimators=518, learning_rate=0.049405517082973915, depth=10,\n",
    "                                l2_leaf_reg=4.134816151740027, grow_policy='Depthwise', random_state=0, silent=True, eval_metric='RMSE')\n",
    "\n",
    "model_ct.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_ct.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out + K-Fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Model\n",
    "def objectiveCT(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10, log=True),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide'])\n",
    "    }\n",
    "\n",
    "    model = ct.CatBoostRegressor(**params, random_state=0, silent=True, eval_metric='RMSE')\n",
    "\n",
    "    # Compute MAE and R2 for analysis\n",
    "    mae_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='r2')\n",
    "\n",
    "    # Store MAE and R2 scores as user attributes\n",
    "    trial.set_user_attr('mae_scores', mae_scores)\n",
    "    trial.set_user_attr('r2_scores', r2_scores)\n",
    "\n",
    "    # Compute neg_root_mean_squared_error for optimization\n",
    "    rmse_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_root_mean_squared_error')\n",
    "    trial.set_user_attr('rmse_scores', rmse_scores)\n",
    "\n",
    "    return rmse_scores.mean()\n",
    "\n",
    "studyCT = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyCT.optimize(objectiveCT, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "try:\n",
    "    best_rmse_scores = studyCT.best_trial.user_attrs['rmse_scores']\n",
    "    best_mae_scores = studyCT.best_trial.user_attrs['mae_scores']\n",
    "    best_r2_scores = studyCT.best_trial.user_attrs['r2_scores']\n",
    "\n",
    "    mean_rmse = np.mean(best_rmse_scores)\n",
    "    std_rmse = np.std(best_rmse_scores)\n",
    "    mean_mae = np.mean(best_mae_scores)\n",
    "    std_mae = np.std(best_mae_scores)\n",
    "    mean_r2 = np.mean(best_r2_scores)\n",
    "    std_r2 = np.std(best_r2_scores)\n",
    "\n",
    "    print(\"Mean RMSE of Best Model:\", mean_rmse)\n",
    "    print(\"Standard Deviation of RMSE of Best Model:\", std_rmse)\n",
    "    print(\"Mean MAE of Best Model:\", mean_mae)\n",
    "    print(\"Standard Deviation of MAE of Best Model:\", std_mae)\n",
    "    print(\"Mean R2 of Best Model:\", mean_r2)\n",
    "    print(\"Standard Deviation of R2 of Best Model:\", std_r2)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"No completed trials yet.\")\n",
    "\n",
    "print('Best hyperparameters:', studyCT.best_params)\n",
    "print('Best RMSE:', studyCT.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ct = ct.CatBoostRegressor(n_estimators=951, learning_rate=0.021075826382483023, depth=10,\n",
    "                                l2_leaf_reg=4.558823001649244, grow_policy='Depthwise', random_state=0, silent=True, eval_metric='RMSE')\n",
    "\n",
    "model_ct.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_ct.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "def objectiveSVM(trial):\n",
    "    params = {\n",
    "        'kernel': trial.suggest_categorical(\"kernel\", ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'C': trial.suggest_float('C', 1e-3, 5, log=True),\n",
    "        'degree': trial.suggest_int('degree', 2, 7),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-3, 0.01, log=True),\n",
    "        'coef0': trial.suggest_float('coef0', 0.1, 5.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = SVR(**params)\n",
    "    model.fit(X_train_sca.values, y_train.values.ravel())\n",
    "    predictions = model.predict(X_test_sca.values)\n",
    "    rmse = root_mean_squared_error(y_test.values.ravel(), predictions)\n",
    "    return rmse\n",
    "\n",
    "studySVM = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studySVM.optimize(objectiveSVM, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print('Best hyperparameters:', studySVM.best_params)\n",
    "print('Best RMSE:', studySVM.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svr = SVR(kernel='poly', C=3.7288560084932603, degree=6, gamma=0.0029604914010939115, coef0=3.5821638281807133)\n",
    "\n",
    "model_svr.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_svr.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out + K-Fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "def objectiveSVM(trial):\n",
    "    params = {\n",
    "        'kernel': trial.suggest_categorical(\"kernel\", ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'C': trial.suggest_float('C', 1e-3, 5, log=True),\n",
    "        'degree': trial.suggest_int('degree', 2, 7),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-3, 0.01, log=True),\n",
    "        'coef0': trial.suggest_float('coef0', 0.1, 5.0, log=True)\n",
    "    }\n",
    "\n",
    "    model = SVR(**params)\n",
    "    \n",
    "    # Compute MAE and R2 for analysis\n",
    "    mae_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='r2')\n",
    "\n",
    "    # Store MAE and R2 scores as user attributes\n",
    "    trial.set_user_attr('mae_scores', mae_scores)\n",
    "    trial.set_user_attr('r2_scores', r2_scores)\n",
    "\n",
    "    # Compute neg_root_mean_squared_error for optimization\n",
    "    rmse_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_root_mean_squared_error')\n",
    "    trial.set_user_attr('rmse_scores', rmse_scores)\n",
    "\n",
    "    return rmse_scores.mean()\n",
    "\n",
    "studySVM = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studySVM.optimize(objectiveSVM, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "try:\n",
    "    best_rmse_scores = studySVM.best_trial.user_attrs['rmse_scores']\n",
    "    best_mae_scores = studySVM.best_trial.user_attrs['mae_scores']\n",
    "    best_r2_scores = studySVM.best_trial.user_attrs['r2_scores']\n",
    "\n",
    "    mean_rmse = np.mean(best_rmse_scores)\n",
    "    std_rmse = np.std(best_rmse_scores)\n",
    "    mean_mae = np.mean(best_mae_scores)\n",
    "    std_mae = np.std(best_mae_scores)\n",
    "    mean_r2 = np.mean(best_r2_scores)\n",
    "    std_r2 = np.std(best_r2_scores)\n",
    "\n",
    "    print(\"Mean RMSE of Best Model:\", mean_rmse)\n",
    "    print(\"Standard Deviation of RMSE of Best Model:\", std_rmse)\n",
    "    print(\"Mean MAE of Best Model:\", mean_mae)\n",
    "    print(\"Standard Deviation of MAE of Best Model:\", std_mae)\n",
    "    print(\"Mean R2 of Best Model:\", mean_r2)\n",
    "    print(\"Standard Deviation of R2 of Best Model:\", std_r2)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"No completed trials yet.\")\n",
    "\n",
    "print('Best hyperparameters:', studySVM.best_params)\n",
    "print('Best RMSE:', studySVM.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svr = SVR(kernel='rbf', C=4.99744826902557, degree=6, gamma=0.009995777060509877, coef0=0.7149170011195296)\n",
    "\n",
    "model_svr.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_svr.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbors Model\n",
    "def objectiveNN(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 8, 15),\n",
    "        'weights': trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        'metric': trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski'])\n",
    "    }\n",
    "\n",
    "    model = KNeighborsRegressor(**params)\n",
    "    model.fit(X_train_sca.values, y_train.values.ravel())\n",
    "    predictions = model.predict(X_test_sca.values)\n",
    "    rmse = root_mean_squared_error(y_test.values.ravel(), predictions)\n",
    "    return rmse\n",
    "\n",
    "studyNN = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyNN.optimize(objectiveNN, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print('Best hyperparameters:', studyNN.best_params)\n",
    "print('Best RMSE:', studyNN.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = KNeighborsRegressor(n_neighbors=15, weights='distance', metric='manhattan')\n",
    "\n",
    "model_nn.fit(X_train_sca.values, y_train.values)\n",
    "y_hat = model_nn.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test, y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test, y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test, y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test,y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out + K-Fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbors Model\n",
    "def objectiveNN(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 8, 15),\n",
    "        'weights': trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        'metric': trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski'])\n",
    "    }\n",
    "\n",
    "    model = KNeighborsRegressor(**params)\n",
    "    \n",
    "    # Compute MAE and R2 for analysis\n",
    "    mae_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='r2')\n",
    "\n",
    "    # Store MAE and R2 scores as user attributes\n",
    "    trial.set_user_attr('mae_scores', mae_scores)\n",
    "    trial.set_user_attr('r2_scores', r2_scores)\n",
    "\n",
    "    # Compute neg_root_mean_squared_error for optimization\n",
    "    rmse_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_root_mean_squared_error')\n",
    "    trial.set_user_attr('rmse_scores', rmse_scores)\n",
    "\n",
    "    return rmse_scores.mean()\n",
    "\n",
    "studyNN= optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyNN.optimize(objectiveNN, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "try:\n",
    "    best_rmse_scores = studyNN.best_trial.user_attrs['rmse_scores']\n",
    "    best_mae_scores = studyNN.best_trial.user_attrs['mae_scores']\n",
    "    best_r2_scores = studyNN.best_trial.user_attrs['r2_scores']\n",
    "\n",
    "    mean_rmse = np.mean(best_rmse_scores)\n",
    "    std_rmse = np.std(best_rmse_scores)\n",
    "    mean_mae = np.mean(best_mae_scores)\n",
    "    std_mae = np.std(best_mae_scores)\n",
    "    mean_r2 = np.mean(best_r2_scores)\n",
    "    std_r2 = np.std(best_r2_scores)\n",
    "\n",
    "    print(\"Mean RMSE of Best Model:\", mean_rmse)\n",
    "    print(\"Standard Deviation of RMSE of Best Model:\", std_rmse)\n",
    "    print(\"Mean MAE of Best Model:\", mean_mae)\n",
    "    print(\"Standard Deviation of MAE of Best Model:\", std_mae)\n",
    "    print(\"Mean R2 of Best Model:\", mean_r2)\n",
    "    print(\"Standard Deviation of R2 of Best Model:\", std_r2)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"No completed trials yet.\")\n",
    "\n",
    "print('Best hyperparameters:', studyNN.best_params)\n",
    "print('Best RMSE:', studyNN.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = KNeighborsRegressor(n_neighbors=8, weights='distance', metric='manhattan')\n",
    "\n",
    "model_nn.fit(X_train_sca.values, y_train.values)\n",
    "y_hat = model_nn.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test, y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test, y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test, y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test,y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveDT(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'splitter': trial.suggest_categorical('splitter', ['best', 'random']),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 5),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
    "        'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 1e-3, 0.5, log=True),\n",
    "        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 1e-3, 1, log=True)\n",
    "    }\n",
    "\n",
    "    model = DecisionTreeRegressor(**params,random_state=0)\n",
    "    model.fit(X_train_sca.values, y_train.values)\n",
    "    predictions = model.predict(X_test_sca.values)\n",
    "    rmse = root_mean_squared_error(y_test.values, predictions)\n",
    "    return rmse\n",
    "\n",
    "studyDT = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyDT.optimize(objectiveDT, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print('Best hyperparameters:', studyDT.best_params)\n",
    "print('Best RMSE:', studyDT.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeRegressor(random_state=0, max_depth=5, splitter='best', min_samples_split=3,\n",
    "                                 min_samples_leaf=15, min_weight_fraction_leaf=0.014001513031818463, min_impurity_decrease=0.002364384066963733)\n",
    "\n",
    "model_dt.fit(X_train_sca.values, y_train.values)\n",
    "y_hat = model_dt.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test, y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test, y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test, y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test,y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out + K-Fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbors Model\n",
    "def objectiveDT(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'splitter': trial.suggest_categorical('splitter', ['best', 'random']),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 5),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
    "        'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 1e-3, 0.5, log=True),\n",
    "        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 1e-3, 1, log=True)\n",
    "    }\n",
    "\n",
    "    model = DecisionTreeRegressor(**params,random_state=0)\n",
    "    \n",
    "    # Compute MAE and R2 for analysis\n",
    "    mae_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='r2')\n",
    "\n",
    "    # Store MAE and R2 scores as user attributes\n",
    "    trial.set_user_attr('mae_scores', mae_scores)\n",
    "    trial.set_user_attr('r2_scores', r2_scores)\n",
    "\n",
    "    # Compute neg_root_mean_squared_error for optimization\n",
    "    rmse_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_root_mean_squared_error')\n",
    "    trial.set_user_attr('rmse_scores', rmse_scores)\n",
    "\n",
    "    return rmse_scores.mean()\n",
    "\n",
    "studyDT= optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyDT.optimize(objectiveDT, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "try:\n",
    "    best_rmse_scores = studyDT.best_trial.user_attrs['rmse_scores']\n",
    "    best_mae_scores = studyDT.best_trial.user_attrs['mae_scores']\n",
    "    best_r2_scores = studyDT.best_trial.user_attrs['r2_scores']\n",
    "\n",
    "    mean_rmse = np.mean(best_rmse_scores)\n",
    "    std_rmse = np.std(best_rmse_scores)\n",
    "    mean_mae = np.mean(best_mae_scores)\n",
    "    std_mae = np.std(best_mae_scores)\n",
    "    mean_r2 = np.mean(best_r2_scores)\n",
    "    std_r2 = np.std(best_r2_scores)\n",
    "\n",
    "    print(\"Mean RMSE of Best Model:\", mean_rmse)\n",
    "    print(\"Standard Deviation of RMSE of Best Model:\", std_rmse)\n",
    "    print(\"Mean MAE of Best Model:\", mean_mae)\n",
    "    print(\"Standard Deviation of MAE of Best Model:\", std_mae)\n",
    "    print(\"Mean R2 of Best Model:\", mean_r2)\n",
    "    print(\"Standard Deviation of R2 of Best Model:\", std_r2)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"No completed trials yet.\")\n",
    "\n",
    "print('Best hyperparameters:', studyDT.best_params)\n",
    "print('Best RMSE:', studyDT.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeRegressor(random_state=0, max_depth=8, splitter='best', min_samples_split=2,\n",
    "                                 min_samples_leaf=5, min_weight_fraction_leaf=0.01193235419114749, min_impurity_decrease=0.0010024067265582204)\n",
    "\n",
    "model_dt.fit(X_train_sca.values, y_train.values)\n",
    "y_hat = model_dt.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test, y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test, y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test, y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test,y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveMLP(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f'n_units_{i}', 1, 256))\n",
    "\n",
    "    # Activation function choice\n",
    "    activation = trial.suggest_categorical('activation', ['identity', 'tanh', 'logistic', 'relu'])\n",
    "\n",
    "    # Regularization\n",
    "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Solver\n",
    "    solver = trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam'])\n",
    "\n",
    "    model = MLPRegressor(\n",
    "        random_state=0,\n",
    "        hidden_layer_sizes=tuple(layers),\n",
    "        activation=activation, \n",
    "        solver=solver,\n",
    "        alpha=l2_reg,  # L2 regularization parameter\n",
    "        max_iter=10000000,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_sca.values, y_train.values.ravel())\n",
    "    predictions = model.predict(X_test_sca.values)\n",
    "    rmse = root_mean_squared_error(y_test.values.ravel(), predictions)\n",
    "    return rmse\n",
    "\n",
    "studyMLP = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyMLP.optimize(objectiveMLP, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print('Best hyperparameters:', studyMLP.best_params)\n",
    "print('Best RMSE:', studyMLP.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP = MLPRegressor(\n",
    "        random_state=0,\n",
    "        hidden_layer_sizes=[62, 72],\n",
    "        activation='relu', \n",
    "        solver='adam',\n",
    "        alpha=1.1635523810951026e-05,  # L2 regularization parameter\n",
    "        max_iter=10000000,\n",
    "    )\n",
    "\n",
    "model_MLP.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_MLP.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHapley Additive exPlanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model_MLP.predict, X_train_sca.sample(500, random_state=0))\n",
    "shap_values = explainer.shap_values(X_test_sca)\n",
    "shap.summary_plot(shap_values, X_test_sca, plot_size=(5, 5), show=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-Out + K-Fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveMLP(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f'n_units_{i}', 1, 256))\n",
    "\n",
    "    # Activation function choice\n",
    "    activation = trial.suggest_categorical('activation', ['identity', 'tanh', 'logistic', 'relu'])\n",
    "\n",
    "    # Regularization\n",
    "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Solver\n",
    "    solver = trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam'])\n",
    "\n",
    "    model = MLPRegressor(\n",
    "        random_state=0,\n",
    "        hidden_layer_sizes=tuple(layers),\n",
    "        activation=activation, \n",
    "        solver=solver,\n",
    "        alpha=l2_reg,  # L2 regularization parameter\n",
    "        max_iter=10000000,\n",
    "    )\n",
    "    \n",
    "    # Compute MAE and R2 for analysis\n",
    "    mae_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='r2')\n",
    "\n",
    "    # Store MAE and R2 scores as user attributes\n",
    "    trial.set_user_attr('mae_scores', mae_scores)\n",
    "    trial.set_user_attr('r2_scores', r2_scores)\n",
    "\n",
    "    # Compute neg_root_mean_squared_error for optimization\n",
    "    rmse_scores = -cross_val_score(model, X_train_sca.values, y_train.values.ravel(), n_jobs=-1, cv=KFold(n_splits=5, shuffle=True, random_state=0), scoring='neg_root_mean_squared_error')\n",
    "    trial.set_user_attr('rmse_scores', rmse_scores)\n",
    "\n",
    "    return rmse_scores.mean()\n",
    "\n",
    "studyMLP = optuna.create_study(direction='minimize', sampler=TPESampler(seed=0))\n",
    "studyMLP.optimize(objectiveMLP, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "try:\n",
    "    best_rmse_scores = studyMLP.best_trial.user_attrs['rmse_scores']\n",
    "    best_mae_scores = studyMLP.best_trial.user_attrs['mae_scores']\n",
    "    best_r2_scores = studyMLP.best_trial.user_attrs['r2_scores']\n",
    "\n",
    "    mean_rmse = np.mean(best_rmse_scores)\n",
    "    std_rmse = np.std(best_rmse_scores)\n",
    "    mean_mae = np.mean(best_mae_scores)\n",
    "    std_mae = np.std(best_mae_scores)\n",
    "    mean_r2 = np.mean(best_r2_scores)\n",
    "    std_r2 = np.std(best_r2_scores)\n",
    "\n",
    "    print(\"Mean RMSE of Best Model:\", mean_rmse)\n",
    "    print(\"Standard Deviation of RMSE of Best Model:\", std_rmse)\n",
    "    print(\"Mean MAE of Best Model:\", mean_mae)\n",
    "    print(\"Standard Deviation of MAE of Best Model:\", std_mae)\n",
    "    print(\"Mean R2 of Best Model:\", mean_r2)\n",
    "    print(\"Standard Deviation of R2 of Best Model:\", std_r2)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"No completed trials yet.\")\n",
    "\n",
    "print('Best hyperparameters:', studyMLP.best_params)\n",
    "print('Best RMSE:', studyMLP.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP = MLPRegressor(\n",
    "        random_state=0,\n",
    "        hidden_layer_sizes=[198, 20],\n",
    "        activation='logistic', \n",
    "        solver='adam',\n",
    "        alpha=1.4997590316080432e-05,  # L2 regularization parameter\n",
    "        max_iter=10000000,\n",
    "    )\n",
    "\n",
    "model_MLP.fit(X_train_sca.values, y_train.values.ravel())\n",
    "y_hat = model_MLP.predict(X_test_sca.values)\n",
    "print(f'R**2: {r2_score(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean absolute error (MAE): {mean_absolute_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Mean square error (MSE): {mean_squared_error(y_test.values.ravel(), y_hat)}')\n",
    "print(f'Root Mean Square error (RMSE): {root_mean_squared_error(y_test.values.ravel(),y_hat)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
